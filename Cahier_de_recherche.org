#+title: Cahier De Recherche
#+author: Djoser SIMEU

* mercredi 31 janvier 2024
** Prise en mains du repertoire gitHub
+ Fork du repertoire https://github.com/fredgrub/scheduling-simulator
+ Prise main de Docker
*** DONE :: Mettre à jour la version de python 3.9.12  -> 3.9.2
*** DONE :: Mieux comprendre l'utilisation de conteneurs docker
** Erreurs rencontrée :
+ You may still be able to access the file from the browser:https://drive.google.com/uc?id=1MkSKglrQeI8rEO9hbr-7kYEvSHhrpFKG but Gdown can't. Please check connections and permissions.
  + commande : sudo docker run --rm -v "$(pwd)":/workspaces/scheduling-simulator/ build-simgrid bash -c "python /workspaces/scheduling-simulator/initialize.py"
+ après l'éxécution de la commande : nohup python src/simulator/simulator.py & , le dossier data censé contenir les données de la simulation est vide.
  + donc la commande python /src/regressor/regressor.py échoue.
**** DONE :: regler l'erreur et comprendre la synthax de docker run



* vendredi 2 février
** Code modification
+ line 388 : src/simulator/trial_simulator.c -> commentary (todo[i]->name problem)
+ line 677 : src/tester/sched-simulator-runtime.c ->commentary (todo[i]->name problem)
+ line 624 : src/tester/sched-simulator-backfilling.c -> commentary (todo[i]->name problem)
*** DONE :: Understand why the todo[i]->name problem occur
** Files understanding
*** simulator.py
+ S = initial state of the HPC
+ Q = sets of jobs to be schedule into the HPC
+ SIMULATION_PARAMETERS :
  + number_of_tuples : number of binome S-Q
  + number_of_trials : number of time we schedule the jobs in Q
  + size_of_S : number of jobs in S
  + size_of_Q : number of jobs in Q
**** Simulator attributes
+ _jobs_S : the initial state of the simulated HPC before scheduling
+ _jobs_Q : the set of jobs to be schedule
***** DONE :: Verifier les autres attributs avec Danilo si possible
**** Simulator constructor
+ workload : workload model used to define the simulator
***** DONE :: Understand what does mean the arguments deployment, cluster and fixed_seed
**** Simulator Methods
***** get_workload_info(self)
initialize the number of jobs and processors by reading the workload given to the constructor
****** DONE :: What does mean model_jobs?
***** store_tuple(self, index)
store in _jobs_S and _jobs_Q jobs stored in model_jobs as it said in the paper with M, to apply this we used get_random_index() to define the begining of the set M.
+ Remark : _jobs_S and _jobs_Q -> ["p"]["q"]["r"]
+ Remark : nodes = processors
***** create_permutation (self, index, shuffled_Q)
As it's explain on the paper, we compute |Q| permutation of the set Q to construct multple branch of simulation.
****** DONE :: What does mean a branch of simulation?
***** schedule_trials(self)
Create a a new permutation for each trial
***** compute_AVGbsld(self,index)
Apply the computation of the score(j) based on the equation 3 of the paper
****** DONE :: more Understand the function
***** simulate(self)
call all the functions described previously to compute the simulation.
*** regressor.py
**** Regressor Attributes
+ functions : list of functions used for the multiple linear regression
+ data_set : dataset on which we compute the regression
**** Regressor Methods
***** _compute_weights(self)
compute the weights for the regression by computing 1/(p*q) for all the enteries of the dataset
***** _fit_function(self,function)
Fit the function given as parameter to the dataset by using scipy.optimize.curve_fit
****** TODO :: Understand what does curve_fit
***** _predict_y(self, function, optimal_parameters)
Applying the function given as parameter to the dataset and return the result in an array
***** _compute_mae(self, predicted_y)
compute the mean absolute error of the prediction with the attribute score of the dataset
***** regression(self, output_file, include_covariance=False)
apply the regression with all the functions contain by the object Regressor and write the result into a file
* mercredi 7 février
** File understanding
*** tester.py
**** workload_experiments(workloads, policies, sim_type)
***** Parameters
+ parameter workloads : an array of string which represent in which represent the workloads used to based our simulation
  + possible values :  ["CTC-SP2", "SDSC-BLUE", "LUBLIN 256"]
+ parameter policies : array of string which represent the policies used to schedule the jobs in Q in our experiments
  + possible values : ["FCFS", "WFP3", "UNICEF", "SPT", "SAF", "F2", "LIN", "QDR", "CUB", "QUA", "QUI", "SEX"]
+ parameter sim_type : an array of string which represent the type of simulator we want to use in our simulation
  + possible values : ["ACTUAL", "ESTIMATED"]
****** DONE :: knowing the role of each workload and the particularity of each simulation type
***** Function
+ incomprehension line 98-99 tester.py
+ 1 : strat by collecting informations about the workloads and the type of simulator used
+ 2 : defining a dataframe slowdown where to store all slowdowns from all experiments
+ 3.1 : Defining S and Q from the choosen workload as it's done in simulator.py
+ 3.2 : In the case where the type of simulator used is not "ACTUAL" we must additionally used the attribute ~p which represent the estimated job's processing time
+ 4 : Compute the scheduling experiment of Q for each policy in the parameter policies by the using of the method subprocess.run
+ 5 : write all the slowdowns computed during the experiment in a csv file
** Problem
*** DONE :: Simulation
When I want to launch the simulation by the command python tester.py the simulation didn't occurs and reapeat the same line  : [1295866.000000] [ker_engine/INFO] 2836 actors are still running, waiting for something.
+ Jean Francois said to me :
  + the simulation must start at 0 but in our case the simulation start at 1295866 so it's strange, the cause can be an error in  the end of the simulation. May be the problem can occurs durring the cloture of the simulation.
+ head of err.log :
#+begin_example
[0.000000] [surf_parse/INFO] You're using a v4.0 XML file (/home/djosersimeu/documents/m1_mosig/internship/workspaces/scheduling-simulator/data/platforms/plat_day.xml) while the current standard is v4.1 That's fine, the new version is backward compatible.

Use simgrid_update_xml to update your file automatically to get rid of this warning. This program is installed automatically with SimGrid, or available in the tools/ directory of the source archive.
[0.000000] [surf_parse/INFO] You're using a v4.0 XML file (/home/djosersimeu/documents/m1_mosig/internship/workspaces/scheduling-simulator/data/applications/deployment_ctcsp2.xml) while the current standard is v4.1 That's fine, the new version is backward compatible.

Use simgrid_update_xml to update your file automatically to get rid of this warning. This program is installed automatically with SimGrid, or available in the tools/ directory of the source archive.
[1295866.000000] ./src/kernel/EngineImpl.cpp:851: [ker_engine/CRITICAL] Oops! Deadlock or code not perfectly clean.
[1295866.000000] [ker_engine/INFO] 2836 actors are still running, waiting for something.
[1295866.000000] [ker_engine/INFO] Legend of the following listing: "Actor <pid> (<name>@<host>): <status>"
[1295866.000000] [ker_engine/INFO] Actor 1 (master@node-0) simcall Simcall::RUN_BLOCKING
#+end_example
+ nothing in out.log

**** part of the problem solved
un-commentation of the line commented 02/02/2024 but replacing todo[i]->name by todo[i]
+ out.log :
Performing scheduling performance test for the workload trace CTC-SP2.
Configuration: ACTUAL
Performing scheduling experiment 1. Number of tasks=2835
+ head err.log :
#+begin_example
[0.000000] [surf_parse/INFO] You're using a v4.0 XML file (/home/djosersimeu/documents/m1_mosig/internship/workspaces/scheduling-simulator/data/platforms/plat_day.xml) while the current standard is v4.1 That's fine, the new version is backward compatible.

Use simgrid_update_xml to update your file automatically to get rid of this warning. This program is installed automatically with SimGrid, or available in the tools/ directory of the source archive.
[0.000000] [surf_parse/INFO] You're using a v4.0 XML file (/home/djosersimeu/documents/m1_mosig/internship/workspaces/scheduling-simulator/data/applications/deployment_ctcsp2.xml) while the current standard is v4.1 That's fine, the new version is backward compatible.

Use simgrid_update_xml to update your file automatically to get rid of this warning. This program is installed automatically with SimGrid, or available in the tools/ directory of the source archive.
[0.000000] [surf_parse/INFO] You're using a v4.0 XML file (/home/djosersimeu/documents/m1_mosig/internship/workspaces/scheduling-simulator/data/platforms/plat_day.xml) while the current standard is v4.1 That's fine, the new version is backward compatible.

Use simgrid_update_xml to update your file automatically to get rid of this warning. This program is installed automatically with SimGrid, or available in the tools/ directory of the source archive.
[0.000000] [surf_parse/INFO] You're using a v4.0 XML file (/home/djosersimeu/documents/m1_mosig/internship/workspaces/scheduling-simulator/data/applications/deployment_ctcsp2.xml) while the current standard is v4.1 That's fine, the new version is backward compatible.
#+end_example
**** DONE :: Use nix (ask Dorian)
**** DONE :: Find the computation of VIF
+ In the method _fit_function(self,function) regressor.py line 77 by the call :
    scipy.optimize.curve_fit(
            function,
            (self.data_set["p"], self.data_set["q"], self.data_set["r"]),
            self.data_set["score"],
            sigma=self._compute_weights(),
            absolute_sigma=True,
        )
***** DONE :: reading curve_fit documentation : https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html
* vendredi 9 février
** Meeting with Danilo
*** Sucessing to run the simulation
Danilo send to me the file simgrid.nix which allow me to configure my nix-env withe the right version of simgrid by the command
#+begin_example
nix-shell simgrid.nix
#+end_example
Now I don't need to run initialize.py, i only need to call make in the directories src/tester and src/simulator
*** Genetique algorithm
To increase the precision and the speed of the computation of the targets given to the regressor for his learning, Danilo have implement a genetic algorithm which compute the best permutation of the set of jobs Q, the metric used to compare the permutation during the genetic algo is the AVGBoundedSlowdown.
**** DONE :: read the paper on the genetic algorithm (https://webmail.etu.univ-grenoble-alpes.fr/service/home/~/?auth=co&loc=fr&id=29065&part=2)
+ GA approche to solve RCPSP
**** DONE :: fork the branch https://github.com/fredgrub/scheduling-simulator/tree/dcsantos/genetic_algorithm_dataset_creation into my repositoty
**** DONE :: Implement the method save_score_distribution
***** DONE :: Compute the score of each jobs in the permutation find at the end of genetic algo by the method (rankof the jobs)/(number total of jobs in Q)
***** DONE :: Write the score associate to each jobs on the trainnig data file
**** DONE :: Find a way to define a stop criterion for the number of iteration of the genetic algorithm
* mercredi 14 février
** Preparation magisterial presentation
*** DONE :: The online job scheduling problem can be defined as an NP complete problem?
*** DONE :: Which option is better between talk more about simgrid or talk more about our implementation of the scheduling simulator?
*** DONE :: In the multiple linear regression model the family of functions represent in our case the set of function Lin, Qdr, Cub, Qua ...? And at the end we choose the one which have the best performance?
*** DONE :: Do we loose in explainabilty by using polynomiale features?
*** DONE :: In our simulation how many cores do we have?
*** DONE :: Do we use the same data in trainnig of the models and in the tester.c implementation?
*** DONE :: Data used come from real HPC plateform trace?
*** DONE :: How to define the average bounded slowdown with simple terms ?
* Vendredi 16 février
** Advecement on the Gen algo implementation
*** DONE :: Concatenate the dataset genrerated by the algo to construct our train dataset
+ Adding in simulator the attribute : _global_training_data_path = SIMULATION_DIR / "training-data"/ "global_training_data.csv"
+ Adding in simulator the attribute : self.global_data=open(self._global_training_data_path,"w+")
+ Adding in regressor the global variable : TRAINING_DIR = pathlib.Path(__file__).parent.parent / "simulator" / "training-data"
  + using it : SCORE_DISTRIBUTION = TRAINING_DIR / "global_training_data.csv"
*** DONE :: Define a way to stop the learning of the gen algo
** DONE ::posible utilisation d'une recherche profonde -> Gen algo
** DONE ::latin hypercube for the initialization of the population
** DONE ::Grid Search algo hyper parameter = nb gen , initiaalisation de la population
** DONE ::Jeu experimentale python simulator.py -random/-lhs
| tuple | random |  lhs |              |
|     1 |    512 |  450 |              |
|     2 |     30 |   25 |              |
|    .. |    ... |...   |              |
|    10 |    250 | 2520 | nb_gen = 500 |
metric = Average bounded slowdown
*** DONE :: find a way to use latin hypercube (agrparse)
#+begin_example
for j in range(0, self.population_size):
            self._parents_indices[j] = np.arange(self.size_of_Q)
            shuffle(self._parents_indices[j])
#+end_example
to replace if we use the option -lhs:
#+begin_example
def initialize_population_indexes(self):
        #if self._current_generation == 0:
        self._parents_indices = np.empty(shape=(self.population_size, self.size_of_Q), dtype=int)
       #print(self._parents_indices[0])
        if args.hypercube :
            sampler= qmc.LatinHypercube(d=self.size_of_Q)
            lhs=sampler.random(n=self.population_size)
            for indiv in range (0,self.population_size):
                prob = lhs[indiv]
                copy=[]


                for i in range ( 0,self.size_of_Q):

                    idx=0
                    p=random()

                    while (np.isin(idx,self._parents_indices[indiv]) or p>prob[i]) and idx<self.size_of_Q :
                        idx=idx+1
                        p=random()
                    #print(np.isin(idx,self._parents_indices[indiv]))
                    self._parents_indices[indiv][i]=idx
                    copy.append(idx)
                    #print(copy.count(idx))
            print(self._parents_indices.shape)


        else:

            for j in range(0, self.population_size):
                self._parents_indices[j] = np.arange(self.size_of_Q)
                shuffle(self._parents_indices[j])


        #else:
        #    self.create_childrens()


#+end_example
*** DONE :: Error triggered : Problem solved, due to multiple time the same value in all the individual of the population
#+begin_example
Generation:  0
(40, 32)
Traceback (most recent call last):
  File "/home/djosersimeu/documents/m1_mosig/internship/workspaces/scheduling-simulator/src/simulator/simulator.py", line 396, in <module>
    simulator.simulate()
  File "/home/djosersimeu/documents/m1_mosig/internship/workspaces/scheduling-simulator/src/simulator/simulator.py", line 340, in simulate
    self.create_childrens()
  File "/home/djosersimeu/documents/m1_mosig/internship/workspaces/scheduling-simulator/src/simulator/simulator.py", line 191, in create_childrens
    self.crossover(_mother, _father, i)
  File "/home/djosersimeu/documents/m1_mosig/internship/workspaces/scheduling-simulator/src/simulator/simulator.py", line 171, in crossover
    while _mother[_m] in _son_heritage_father:
IndexError: index 32 is out of bounds for axis 0 with size 32

#+end_example


** TODO Take a look about jupyter notebook which compute the VIF
* Mercredi 21 février
** Implementation of the grid search
+ Creation of jupyter notebook file "GridSearch.ipynb" where we compute the experimental game
*** Random shuffle evaluation

[[file:./images/graph_gs_random_1.png]]
+ The AVGBoundedSlowdown stabilize for all tuples arround the 60th generations
*** Hypercube shuffle evaluation
[[file:./images/graph_gs_hyper.png]]
+ The AVGBounded slowdown stabilize for all tuples arround 300th generations

*** DONE ::Representation in two dimension of the intial distribution of the pop in the two methods
**** Using PCA dimension reduction
#+begin_example
from sklearn.decomposition import PCA

n_compo=2
pca_h =PCA(n_components=n_compo)
lower_dim_data_h =pca_h.fit_transform(init_pop_h)
pca_r =PCA(n_components=n_compo)
lower_dim_data_r =pca_r.fit_transform(init_pop_r)
#+end_example
No real graphical differences
***** Hypercube:
[[file:./images/scatter_h.png]]
***** Random:
[[file:./images/scatter_r.png]]

**** Using the same method as in simulator.py
No real graphical differences
***** Hypercube :
#+begin_example
sampler= qmc.LatinHypercube(d=2)
lhs=sampler.random(n=size_obs)

res_h_x=list()
res_h_y=list()
for i in range(0,size_obs):
    prob=lhs[i]
    copy=[]
    for j in range(0,2):
        idx=randint(0,size_test - 1)
        p=random()

        while (np.isin(idx,copy) or p>prob[j]) :
            idx=randint(0, size_test - 1)
            p=random()
        copy.append(idx)
    res_h_x.append(copy[0])
    res_h_y.append(copy[1])
#+end_example
[[file:./images/sc.png]]
***** Random :
#+begin_example
from random import shuffle,randint,random
from scipy.stats import qmc
import numpy as np
size_obs=40
size_test=32
test1=np.arange(size_test)
shuffle(test1)
res_r_x=list()
res_r_y=list()
for i in range (0,size_obs):
    shuffle(test1)
    res_r_x.append(test1[0])
    res_r_y.append(test1[1])
#+end_example
[[file:./images/sr.png]]
* vendredi 23 février
** DONE :: Graph with 10 time the same tuples in the two configuration with different seed to ensure reproductibility
+ list of seed : 42 , 23 , 32, 15, 234 , 898 , 747, 45, 14, 1
  Changing the value simulator.py line 78 : seed(42) and line 217 : sampler= qmc.LatinHypercube(d=self.size_of_Q,seed=42)
*** For Ramdom shuffle :
[[file:./images/r_shuffle_seed.png]]
*** For Hypercube shuffle :
[[file:./images/h_shuffle_seed.png]]
** DONE :: Learn how to use Grid'5000
+ Tutorial getting started : https://www.grid5000.fr/w/Getting_Started#Connecting_for_the_first_time
+ Usefull ressource for installing nix in Grid'5000 :https://nix-tutorial.gitlabpages.inria.fr/nix-tutorial/installation.html
* Jeudi 29 février
** First utilisation of Grid5000
+ to connect to the server :
  #+begin_example
  ssh grenoble.g5k
  #+end_example
+ to copy a file/folder into my space in the server :
  #+begin_example
  scp myfile.py g5k:targetfolder
  scp -r myfolder g5k:targetfolder
  #+end_example
+ to run a file :
  #+begin_example
  oarsub -l host=1/core=1 "python3 myfile.py"
  #+end_example
+ to see the advancement of my task:
  #+begin_example
  oarstat -u
  #+end_example
** Advancement
+ Sucessely run simple program as hello.python
+ Sucessely copy paste my workspaces into my Grid5000 env
*** Problems
**** DONE :: Problem in the running of simulator.py :
#+begin_example
Traceback (most recent call last):
  File "/home/dsimeu/public/workspaces/scheduling-simulator/src/simulator/simulator.py", line 444, in <module>
    simulator.simulate()
  File "/home/dsimeu/public/workspaces/scheduling-simulator/src/simulator/simulator.py", line 380, in simulate
    self.create_initial_state(tuple_index)
  File "/home/dsimeu/public/workspaces/scheduling-simulator/src/simulator/simulator.py", line 145, in create_initial_state
    subprocess.run(
  File "/usr/lib/python3.9/subprocess.py", line 505, in run
    with Popen(*popenargs, **kwargs) as process:
  File "/usr/lib/python3.9/subprocess.py", line 951, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/usr/lib/python3.9/subprocess.py", line 1823, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: '/home/dsimeu/public/workspaces/scheduling-simulator/src/simulator/trials_simulator'
#+end_example
+ Advencement the error "No such file or directory" explaination:"libsimgrid.so.3.13 => not found"
#+begin_example
    $ ldd /home/dsimeu/public/workspaces/scheduling-simulator/src/simulator/trials_simulator
	linux-vdso.so.1 (0x00007ffeb05fb000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f3bf10c8000)
	libsimgrid.so.3.13 => not found
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f3bf0ef4000)
	/nix/store/z56jcx3j1gfyk4sv7g8iaan0ssbdkhz1-glibc-2.33-56/lib/ld-linux-x86-64.so.2 => /lib64/ld-linux-x86-64.so.2 (0x00007f3bf1225000)
#+end_example
+ Possible usage of guix :https://guix.gnu.org/manual/fr/html_node/
* vendredi 1 mars
** Sucessfuly run simulator.py on Grid5000 method :
+ reserve a specific node:
#+begin_example
dsimeu@fgrenoble:~/public/workspaces$ oarsub -I -l host=1,walltime=1:45 -t deploy
#+end_example
+ deploy the same distrubtuion as me on the node :
  #+begin_example
  dsimeu@fgrenoble:~/public/workspaces$ kadeploy3 ubuntu2204-min
  #+end_example
+ connecting to the node (in our case dahu-30 given by the previous function)
  #+begin_example
  dsimeu@fgrenoble:~/public/workspaces$ ssh root@dahu-30.grenoble.grid5000.fr
  #+end_example
+ install nix on the node :
  #+begin_example
root@dahu-30:~# sh <(curl -L https://nixos.org/nix/install) --daemon
   #+end_example
+ also install packages which allow us to call nix commands:
  #+begin_example
  root@dahu-30:~# apt install nix-bin
  #+end_example
+ return to our grid5000 env by Ctrl+D
+ copy the workspaces folder wich contain our file to the node's environnement:
  #+begin_example
  dsimeu@fgrenoble:~/public$ rsync -r workspaces root@dahu-30.grenoble.grid5000.fr:
  #+end_example
+ run the nix-shell with our file:
  #+begin_example
  root@dahu-30:~/workspaces# nix-shell simgrid.nix
#+end_example
+ run our file:
  #+begin_example
  [nix-shell:~/workspaces/scheduling-simulator/src/simulator]# python3 simulator.py
#+end_example
** Observations:
Not a big improvement compare to the running in local
*** DONE ::
*** DONE :: How to use multiple node with this method
*** TODO :: How to save a particular configuration to setup nodes
* mercredi 6 mars
** Observation
+ The runtime on Grid 5000 is similar to the runtime on my local machine , maybe it's due to the multiple disk acesses at each iteration of the GA
+ Maybe we can decrease the runtime by using parallel programming methods such as OpenMP
+ Grid 5000 allow us to use external hardware devices in our case it usefull for generating the training dataset with 10 000 observation but it will take approximately the same time as a local running.
** Runtime computation
+ 2 Gen/sec
+ We need 100 000 observations
+ Size of Q = 32
+ Nb tuples needed = 3125
+ Nb Gen = 300
+ Runtimes by tuples = 150 sec
+ Global Runtimes = 130 H 13 min
** TODO :: Olivier Richard solution
+ i can decompose the programm set of tuples to use multiple cores by lauching multipler process in dahu
+ To decompose my task i can use gnu parallel (https://www.grid5000.fr/w/GNU_Parallel)
+ 31 process which each execute 100 tuples and 1 process 25 tuples
+ like that the full running will take 4H 11min
+ use tmux
+ attention concurent acces
** DONE :: Comparaison GA Deep search
+ 10 tuples
+ 2*10 curves ( Ga, DS )
+ slodown in function of time
+ time computation : Averge running tim by generation
* Vendredi 8 mars
** Comparaison GA DS
+ The code here https://github.com/fredgrub/scheduling-simulator/blob/main/src/simulator/simulator.py compute the DS algorithm to find a target priorities
+ Run the DS code and store only new minimum for plotting
+ Because of the time needed to run the DS program with 256 000 trials for 10 tuples, for the moment we compute the result only for 1 tuple
*** DONE :: Paralellize the computation of each tuples for the two experimental contexts (DS,GA)
*** DONE :: Do the same thing with a budget of 1 hour for each context and compare them
* Lundi 11 mars
** Paralellize the computation by tuple
*** DONE :: Use the arg for the seed
*** DONE :: Finding a way to parallelize the computation
One node for GA and one node for DS
**** Genetic algo and Deep Search method
+ 10 tuples to compute
+ 10 cpu with 1 tuples
+ First idea : copy 10 time the directory and run simulator.py and simulator_trials.py in each directories with 10 different seed
***** DONE :: See MPI library
* Mercredi 13 mars
** Creation of the parallel context of execution
+ Creation of multiple folder, each one represent a process which will run in parallel of the others
+ Creation of a script for the execution of the processes
  #+begin_example
  #!/bin/bash

# Tableau contenant les chemins des scripts Python à exécuter
scripts=(
    "scheduling-simulator_DS/scheduling-simulator_DS_1/src/simulator/simulator_trials.py"
    "scheduling-simulator_DS/scheduling-simulator_DS_2/src/simulator/simulator_trials.py"
    "scheduling-simulator_DS/scheduling-simulator_DS_3/src/simulator/simulator_trials.py"
    "scheduling-simulator_DS/scheduling-simulator_DS_4/src/simulator/simulator_trials.py"
    "scheduling-simulator_DS/scheduling-simulator_DS_5/src/simulator/simulator_trials.py"
    "scheduling-simulator_DS/scheduling-simulator_DS_6/src/simulator/simulator_trials.py"
    "scheduling-simulator_DS/scheduling-simulator_DS_7/src/simulator/simulator_trials.py"
    "scheduling-simulator_DS/scheduling-simulator_DS_8/src/simulator/simulator_trials.py"
    "scheduling-simulator_DS/scheduling-simulator_DS_9/src/simulator/simulator_trials.py"
    "scheduling-simulator_DS/scheduling-simulator_DS_10/src/simulator/simulator_trials.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_1/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_2/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_3/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_4/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_5/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_6/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_7/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_8/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_9/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_10/src/simulator/simulator.py"
    # Ajoutez ici les chemins des autres scripts Python que vous souhaitez exécuter
)

# Boucle pour ouvrir 20 terminaux et exécuter les scripts Python
for i in {0..19}
do
    gnome-terminal --command "python3 ${scripts[$i]} ${i}" &
done
#+end_example
** DONE :: Problem, the gnome-terminal instruction is not installed*
** Results of the comparison :
[[file:./images/mean_90_10.png]]
[[file:./images/GAvsDS.png]]
+ As we can see for all the tuple of the evaluation the GA succeed to have a better score than the DS
**
* vendredi 15 mars
** Update of the script:
+ remove the gnome-command by: python3 ${scripts[$i]} ${i} > outputs/output_${i}.txt &
*** DONE :: Z-score
** Writting a script to process the data generation
#+begin_example
#!/bin/bash
sudo-g5k
sh <(curl -L https://nixos.org/nix/install) --daemon --yes
sudo apt install nix-bin -y
#nix-shell simgrid.nix &
# Tableau contenant les chemins des scripts Python à exécuter
scripts=(
    "scheduling-simulator_GA/scheduling-simulator_GA_1/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_2/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_3/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_4/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_5/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_6/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_7/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_8/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_9/src/simulator/simulator.py"
    "scheduling-simulator_GA/scheduling-simulator_GA_10/src/simulator/simulator.py"
    # Ajoutez ici les chemins des autres scripts Python que vous souhaitez exécuter
)

# Boucle pour ouvrir 20 terminaux et exécuter les scripts Python
for i in {0..9}
do
    nix-shell simgrid.nix --command "python3 ${scripts[$i]} ${i} > outputs/output_${i}.txt" &
done

#+end_example
* mercredi 20 mars
** Runnig the generation of the trainning dataset
to generate the training dataset i modified the script previously defined to divide the execution into 32 processes where each one will execute 100 tuples (for each configuration DS an GA)
+ I divided the number of trials in the DS version by 10 to reduce the runtime and because in our observations the DS version find approximatly is minimum in les than 25600 trials in median
+ Now instead of do a deployment we can used the command oarsub -I -l host=1,walltime=1:45
** Beginning of the half report
*** Organization
**** Introduction
**** Preliminary concepts
+ ressources managment in HPC
+ Simgrid
+ scheduling-simulator
+ Genetic Algorithm
+ Grid 5000 ?
+ Multiple linear regressions model
**** Gentic Algorithm deployment
+ finalize the implementation
***** Trying other implementation
+ LatinHypercube
+ Deep search
**** Multiple Linear Regressions model
+ Polynomials of jobs attributes
+ Correlation of jobs attributes
+ Possible solutions
**** Conclusion
*** DONE :: Informations about SimGrid
*** DONE :: Informations about Grid 5000
+ Jusrt a citation : https://www.grid5000.fr/mediawiki/index.php?title=Special:CiteThisPage&page=News&id=102364&wpFormIdentifier=titleform
*** DONE :: Informations about the theorie of the Latin Hypercubes
*** DONE :: Informations about the part of Multiple linear regressions model
* vendredi 22 mars
** DONE :: Collect data from the Grid 5000 front end to save it locally for the GA version
+ Process stop as tuple 82 -83 so for the moment we have 83968 observations of jobs

** TODO :: Prepare the deep search running process in exec_script.sh
** TODO :: Read documentation about the functionnement of the multiple linear regressions before trainning
*** VIF computation
+ read_score_distribution :
  read a csv file which contain the scores, define the cols name to p,q,r and score and call convert_temporal_dataon the dataframe
+ convert_temporal_data:
  divide the temporal informations (p and r) by 3600 (why?
+ compute_vif :
  call the methode variance_inflation_factor for each of the observation of the array (see: https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)


** TODO :: do sh exec_script.sh i
** scp g5k:grenoble/public/... [destination in local]
